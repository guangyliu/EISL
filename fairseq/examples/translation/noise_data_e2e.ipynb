{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ongoing-globe",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'train.tgt'\n",
    "f_valid_en = open('./e2e/e2e-dataset/'+file_name)\n",
    "lines_valid = f_valid_en.readlines()\n",
    "f_valid_en.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "described-token",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcnt = {}\n",
    "for line in lines_valid:\n",
    "    for word in line.split():\n",
    "        if word in wordcnt.keys():\n",
    "            wordcnt[word] += 1\n",
    "        else:\n",
    "            wordcnt[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "parental-america",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_list_all = list(wordcnt.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "present-forwarding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 340298\n",
      "2 302045\n",
      "3 264332\n",
      "4 226941\n",
      "5 188697\n",
      "6 150129\n",
      "7 113106\n",
      "8 75517\n",
      "9 37655\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# importing shutil module \n",
    "import shutil\n",
    "import numpy as np\n",
    "import random\n",
    "# path\n",
    "path = 'ori_multi30k/'\n",
    "for drop in range(1,10):\n",
    "    new_path = \"multi30k_drop\"+str(drop)\n",
    "    drop_prob = drop  * 0.1\n",
    "    # List files and directories\n",
    "    # in '/home/User/Documents'\n",
    "    folder = os.path.exists(new_path)\n",
    "    if not folder:                   #判断是否存在文件夹如果不存在则创建为文件夹\n",
    "        os.makedirs(new_path)            #makedirs 创建文件时如果路径不存在会创建这个路径\n",
    "    for file in os.listdir(path):\n",
    "        if file_name in file:\n",
    "            continue\n",
    "        source = os.path.join(path,file)\n",
    "        destination = os.path.join(new_path,file)\n",
    "        if os.path.exists(destination):\n",
    "            continue\n",
    "    # Copy the content of\n",
    "    # source to destination\n",
    "        dest = shutil.copyfile(source, destination)\n",
    "\n",
    "    file = file_name\n",
    "    new_file = './'+new_path+'/'+file\n",
    "    if os.path.exists(new_file):\n",
    "        os.remove(new_file)\n",
    "    with open(new_file, 'a') as the_file:\n",
    "        drop_lines = []\n",
    "        sum_len = 0\n",
    "        for line in lines_valid:\n",
    "            word_list = line.split()\n",
    "            sen_len = len(word_list)\n",
    "            keep = np.random.rand(len(word_list)) > drop_prob\n",
    "            sent = [w for j, w in enumerate(word_list) if keep[j]]\n",
    "            the_file.write(' '.join(sent)+'\\n')\n",
    "            sum_len+= len(sent)\n",
    "        print(drop,sum_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "emerging-vault",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05\n",
      "0.1\n",
      "0.15\n",
      "0.2\n",
      "0.25\n",
      "0.3\n",
      "0.35\n",
      "0.4\n",
      "0.45\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "for drop in range(5,55,5):\n",
    "    print(drop/100.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "essential-crack",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./e2e/blank5/train.tgt 5 829516\n",
      "./e2e/blank10/train.tgt 10 829516\n",
      "./e2e/blank15/train.tgt 15 829516\n",
      "./e2e/blank20/train.tgt 20 829516\n",
      "./e2e/blank25/train.tgt 25 829516\n",
      "./e2e/blank30/train.tgt 30 829516\n",
      "./e2e/blank35/train.tgt 35 829516\n",
      "./e2e/blank40/train.tgt 40 829516\n",
      "./e2e/blank45/train.tgt 45 829516\n",
      "./e2e/blank50/train.tgt 50 829516\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# importing shutil module \n",
    "import shutil\n",
    "import numpy as np\n",
    "import random\n",
    "# path\n",
    "path = 'e2e/ori'\n",
    "for drop in range(5,55,5):\n",
    "    new_path = \"e2e/blank\"+str(drop)\n",
    "    drop_prob = drop/100.\n",
    "    # List files and directories\n",
    "    # in '/home/User/Documents'\n",
    "    folder = os.path.exists(new_path)\n",
    "    if not folder:                   #判断是否存在文件夹如果不存在则创建为文件夹\n",
    "        os.makedirs(new_path)            #makedirs 创建文件时如果路径不存在会创建这个路径\n",
    "    for file in os.listdir(path):\n",
    "        if file_name in file:\n",
    "            continue\n",
    "        source = os.path.join(path,file)\n",
    "        destination = os.path.join(new_path,file)\n",
    "        if os.path.exists(destination):\n",
    "            continue\n",
    "    # Copy the content of\n",
    "    # source to destination\n",
    "        dest = shutil.copyfile(source, destination)\n",
    "\n",
    "    file = file_name\n",
    "    new_file = './'+new_path+'/'+file\n",
    "    if os.path.exists(new_file):\n",
    "        os.remove(new_file)\n",
    "    with open(new_file, 'a') as the_file:\n",
    "        drop_lines = []\n",
    "        sum_len = 0\n",
    "        for line in lines_valid:\n",
    "            word_list = line.split()\n",
    "            sen_len = len(word_list)\n",
    "            keep = np.random.rand(len(word_list)) > drop_prob\n",
    "    #         sent = [w for j, w in enumerate(word_list) if keep[j]]\n",
    "            sent = []\n",
    "            for j,w in enumerate(word_list):\n",
    "                if keep[j]:\n",
    "                    sent.append(w)\n",
    "                else:\n",
    "                    sent.append('unk')\n",
    "    #         sent = [word_list[j] for j,k in enumerate(keep) if k else '<blank>']\n",
    "            the_file.write(' '.join(sent)+'\\n')\n",
    "            sum_len+= len(sent)\n",
    "        print(new_file,drop,sum_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "auburn-bread",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['restaurant', 'in', 'centre.', 'an', 'English', 'the', 'city', 'Aromi', 'is']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "lovely-inclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# importing shutil module \n",
    "import shutil\n",
    "import numpy as np\n",
    "import random\n",
    "# path\n",
    "path = 'e2e/ori'\n",
    "for drop in range(2,11):\n",
    "    new_path = \"e2e/shuffle\"+str(drop)\n",
    "    drop_prob = drop\n",
    "    # List files and directories\n",
    "    # in '/home/User/Documents'\n",
    "    folder = os.path.exists(new_path)\n",
    "    if not folder:                   #判断是否存在文件夹如果不存在则创建为文件夹\n",
    "        os.makedirs(new_path)            #makedirs 创建文件时如果路径不存在会创建这个路径\n",
    "    for file in os.listdir(path):\n",
    "        if file_name in file:\n",
    "            continue\n",
    "        source = os.path.join(path,file)\n",
    "        destination = os.path.join(new_path,file)\n",
    "        if os.path.exists(destination):\n",
    "            continue\n",
    "    # Copy the content of\n",
    "    # source to destination\n",
    "        dest = shutil.copyfile(source, destination)\n",
    "\n",
    "    file = file_name\n",
    "    new_file = './'+new_path+'/'+file\n",
    "    if os.path.exists(new_file):\n",
    "        os.remove(new_file)\n",
    "    with open(new_file, 'a') as the_file:\n",
    "        drop_lines = []\n",
    "        sum_len = 0\n",
    "\n",
    "        for line in lines_valid:\n",
    "            drop_prob = drop\n",
    "            sent = []\n",
    "            word_list = line.split()\n",
    "            sen_len = len(word_list)\n",
    "            sort = list(range(sen_len))\n",
    "            random.shuffle(sort)\n",
    "            if drop_prob > len(word_list) - 1:\n",
    "                if len(word_list)  >= 2 :\n",
    "                    drop_prob = len(word_list) -1\n",
    "                else:\n",
    "                    drop_prob = 0\n",
    "            for i in range(drop_prob):\n",
    "                tmp = word_list[sort[i]]\n",
    "                word_list[sort[i]] = word_list[sort[i+1]]\n",
    "                word_list[sort[i+1]] = tmp\n",
    "    #         for i, word in enumerate(word_list):\n",
    "    #             sent.append(word_list[sort[i]])\n",
    "            the_file.write(' '.join(word_list)+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "injured-homework",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# importing shutil module \n",
    "import shutil\n",
    "import numpy as np\n",
    "import random\n",
    "# path\n",
    "path = 'e2e/ori'\n",
    "drop = 'all'\n",
    "\n",
    "new_path = \"e2e/shuffle\"+str(drop)\n",
    "# List files and directories\n",
    "# in '/home/User/Documents'\n",
    "folder = os.path.exists(new_path)\n",
    "if not folder:                   #判断是否存在文件夹如果不存在则创建为文件夹\n",
    "    os.makedirs(new_path)            #makedirs 创建文件时如果路径不存在会创建这个路径\n",
    "for file in os.listdir(path):\n",
    "    if file_name in file:\n",
    "        continue\n",
    "    source = os.path.join(path,file)\n",
    "    destination = os.path.join(new_path,file)\n",
    "    if os.path.exists(destination):\n",
    "        continue\n",
    "# Copy the content of\n",
    "# source to destination\n",
    "    dest = shutil.copyfile(source, destination)\n",
    "\n",
    "file = file_name\n",
    "new_file = './'+new_path+'/'+file\n",
    "if os.path.exists(new_file):\n",
    "    os.remove(new_file)\n",
    "with open(new_file, 'a') as the_file:\n",
    "    drop_lines = []\n",
    "    sum_len = 0\n",
    "\n",
    "    for line in lines_valid:\n",
    "        new_word_sent = []\n",
    "        sent = []\n",
    "        word_list = line.split()\n",
    "        sen_len = len(word_list)\n",
    "        sort = list(range(sen_len))\n",
    "        random.shuffle(sort)\n",
    "        for i in range(sen_len):\n",
    "            new_word_sent.append(word_list[sort[i]])\n",
    "        the_file.write(' '.join(new_word_sent)+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "human-abortion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'man',\n",
       " 'in',\n",
       " 'shorts',\n",
       " 'and',\n",
       " 'a',\n",
       " 'hawaiian',\n",
       " 'shirt',\n",
       " 'leans',\n",
       " 'over',\n",
       " 'the',\n",
       " 'rail',\n",
       " 'of',\n",
       " 'a',\n",
       " 'pilot',\n",
       " 'boat',\n",
       " ',',\n",
       " 'with',\n",
       " 'fog',\n",
       " 'and',\n",
       " 'mountains',\n",
       " 'in',\n",
       " 'the',\n",
       " 'background',\n",
       " '.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "yellow-proceeding",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['leans',\n",
       " 'shorts',\n",
       " 'pilot',\n",
       " 'and',\n",
       " '.',\n",
       " 'in',\n",
       " 'over',\n",
       " 'boat',\n",
       " 'a',\n",
       " 'mountains',\n",
       " 'a',\n",
       " 'fog',\n",
       " 'shirt',\n",
       " ',',\n",
       " 'hawaiian',\n",
       " 'the',\n",
       " 'a',\n",
       " 'rail',\n",
       " 'background',\n",
       " 'in',\n",
       " 'the',\n",
       " 'of',\n",
       " 'with',\n",
       " 'and',\n",
       " 'man']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_word_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "limiting-montreal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 377534\n",
      "2 377534\n",
      "3 377534\n",
      "4 377534\n",
      "5 377534\n",
      "6 377534\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# importing shutil module \n",
    "import shutil\n",
    "import numpy as np\n",
    "import random\n",
    "# path\n",
    "path = 'ori_multi30k/'\n",
    "for drop in range(1,7):\n",
    "    new_path = \"multi30k_sub\"+str(drop)\n",
    "    drop_prob = drop  * 0.1\n",
    "    # List files and directories\n",
    "    # in '/home/User/Documents'\n",
    "    folder = os.path.exists(new_path)\n",
    "    if not folder:                   #判断是否存在文件夹如果不存在则创建为文件夹\n",
    "        os.makedirs(new_path)            #makedirs 创建文件时如果路径不存在会创建这个路径\n",
    "    for file in os.listdir(path):\n",
    "        if file_name in file:\n",
    "            continue\n",
    "        source = os.path.join(path,file)\n",
    "        destination = os.path.join(new_path,file)\n",
    "        if os.path.exists(destination):\n",
    "            continue\n",
    "    # Copy the content of\n",
    "    # source to destination\n",
    "        dest = shutil.copyfile(source, destination)\n",
    "\n",
    "    file = file_name\n",
    "    new_file = './'+new_path+'/'+file\n",
    "    if os.path.exists(new_file):\n",
    "        os.remove(new_file)\n",
    "    with open(new_file, 'a') as the_file:\n",
    "        drop_lines = []\n",
    "        sum_len = 0\n",
    "        for line in lines_valid:\n",
    "            word_list = line.split()\n",
    "            sen_len = len(word_list)\n",
    "            keep = np.random.rand(len(word_list)) > drop_prob\n",
    "    #         sent = [w for j, w in enumerate(word_list) if keep[j]]\n",
    "            sent = []\n",
    "            for j,w in enumerate(word_list):\n",
    "                if keep[j]:\n",
    "                    sent.append(w)\n",
    "                else:\n",
    "                    rand = np.random.randint(len(word_list_all))\n",
    "                    sent.append(word_list_all[rand])\n",
    "    #         sent = [word_list[j] for j,k in enumerate(keep) if k else '<blank>']\n",
    "            the_file.write(' '.join(sent)+'\\n')\n",
    "            sum_len+= len(sent)\n",
    "        print(drop,sum_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "african-springfield",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "equal-objective",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['asphalt',\n",
       " 'decorated',\n",
       " 'slo@@',\n",
       " 'benches',\n",
       " 'chers',\n",
       " 'gate',\n",
       " 'collared',\n",
       " 'ka@@',\n",
       " 'set',\n",
       " 'wakeboard',\n",
       " 'dding',\n",
       " 'fabri@@',\n",
       " 'toy',\n",
       " 'c-@@',\n",
       " 'pilot',\n",
       " 'boat',\n",
       " 'spre@@',\n",
       " 'potat@@',\n",
       " 'teeth',\n",
       " 'canal',\n",
       " 'visiting',\n",
       " 'dough',\n",
       " 'coffee',\n",
       " 'condu@@',\n",
       " 'cat',\n",
       " 'leggings']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "corresponding-sterling",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_file numpy as np\n",
    "import os \n",
    "path = \"multi30k_blank2\"\n",
    "drop_prob = 0.2\n",
    "folder = os.path.exists(path)\n",
    "if not folder:                   #判断是否存在文件夹如果不存在则创建为文件夹\n",
    "    os.makedirs(path)            #makedirs 创建文件时如果路径不存在会创建这个路径\n",
    "with open('./'+path+'/'+file, 'a') as the_file:\n",
    "    drop_lines = []\n",
    "    sum_len = 0\n",
    "    for line in lines_valid:\n",
    "        word_list = line.split()\n",
    "        sen_len = len(word_list)\n",
    "        keep = np.random.rand(len(word_list)) > drop_prob\n",
    "#         sent = [w for j, w in enumerate(word_list) if keep[j]]\n",
    "        sent = []\n",
    "        for j,w in enumerate(word_list):\n",
    "            if keep[j]:\n",
    "                sent.append(w)\n",
    "            else:\n",
    "                sent.append('<blank>')\n",
    "#         sent = [word_list[j] for j,k in enumerate(keep) if k else '<blank>']\n",
    "        the_file.write(' '.join(sent)+'\\n')\n",
    "        sum_len+= len(word_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "short-vocabulary",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os \n",
    "import random\n",
    "path = \"multi30k_shuffle3\"\n",
    "drop_prob = 3\n",
    "folder = os.path.exists(path)\n",
    "if not folder:                   #判断是否存在文件夹如果不存在则创建为文件夹\n",
    "    os.makedirs(path)            #makedirs 创建文件时如果路径不存在会创建这个路径\n",
    "with open('./'+path+'/'+file, 'a') as the_file:\n",
    "    drop_lines = []\n",
    "    sum_len = 0\n",
    "    \n",
    "    for line in lines_valid:\n",
    "        sent = []\n",
    "        word_list = line.split()\n",
    "        sen_len = len(word_list)\n",
    "        sort = list(range(sen_len))\n",
    "        random.shuffle(sort)\n",
    "        if drop_prob > len(word_list) - 1:\n",
    "            if len(word_list)  >= 2 :\n",
    "                drop_prob = len(word_list) -1\n",
    "            else:\n",
    "                drop_prob = 0\n",
    "        for i in range(drop_prob):\n",
    "            tmp = word_list[sort[i]]\n",
    "            word_list[sort[i]] = word_list[sort[i+1]]\n",
    "            word_list[sort[i+1]] = tmp\n",
    "#         for i, word in enumerate(word_list):\n",
    "#             sent.append(word_list[sort[i]])\n",
    "        the_file.write(' '.join(word_list)+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "known-heavy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os \n",
    "import random\n",
    "path = \"multi30k_shuffle8_drop2\"\n",
    "drop_prob = 8\n",
    "drop_prob1 = 0.2\n",
    "folder = os.path.exists(path)\n",
    "if not folder:                   #判断是否存在文件夹如果不存在则创建为文件夹\n",
    "    os.makedirs(path)            #makedirs 创建文件时如果路径不存在会创建这个路径\n",
    "with open('./'+path+'/'+file, 'a') as the_file:\n",
    "    drop_lines = []\n",
    "    sum_len = 0\n",
    "    \n",
    "    for line in lines_valid:\n",
    "        sent = []\n",
    "        word_list = line.split()\n",
    "        sen_len = len(word_list)\n",
    "        sort = list(range(sen_len))\n",
    "        random.shuffle(sort)\n",
    "        if drop_prob > len(word_list) - 1:\n",
    "            if len(word_list)  >= 2 :\n",
    "                drop_prob = len(word_list) -1\n",
    "            else:\n",
    "                drop_prob = 0\n",
    "        for i in range(drop_prob):\n",
    "            tmp = word_list[sort[i]]\n",
    "            word_list[sort[i]] = word_list[sort[i+1]]\n",
    "            word_list[sort[i+1]] = tmp\n",
    "#         for i, word in enumerate(word_list):\n",
    "#             sent.append(word_list[sort[i]])\n",
    "        sen_len = len(word_list)\n",
    "        keep = np.random.rand(len(word_list)) > drop_prob1\n",
    "        sent = [w for j, w in enumerate(word_list) if keep[j]]\n",
    "        the_file.write(' '.join(sent)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "married-interim",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'leans',\n",
       " 'a',\n",
       " 'hawaiian',\n",
       " 'shirt',\n",
       " 'with',\n",
       " 'over',\n",
       " 'the',\n",
       " 'of',\n",
       " 'pilot',\n",
       " 'boat',\n",
       " 'rail',\n",
       " 'fo@@',\n",
       " 'g',\n",
       " 'and',\n",
       " 'mountains',\n",
       " 'in',\n",
       " 'background',\n",
       " '.']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "antique-precipitation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a man in shorts and a hawaiian shirt leans over the rail of a pilot boat , with fo@@ g and mountains in the background .\\n'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thorough-slave",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_shuffle(vocab, x, k):   # slight shuffle such that |sigma[i]-i| <= k\n",
    "    base = torch.arange(x.size(0), dtype=torch.float).repeat(x.size(1), 1).t()\n",
    "    inc = (k+1) * torch.rand(x.size())\n",
    "    inc[x == vocab.go] = 0     # do not shuffle the start sentence symbol\n",
    "    inc[x == vocab.pad] = k+1  # do not shuffle end paddings\n",
    "    _, sigma = (base + inc).sort(dim=0)\n",
    "    return x[sigma, torch.arange(x.size(1))]\n",
    "\n",
    "def word_drop(vocab, x, p):     # drop words with probability p\n",
    "    x_ = []\n",
    "    for i in range(x.size(1)):\n",
    "        words = x[:, i].tolist()\n",
    "        keep = np.random.rand(len(words)) > p\n",
    "        keep[0] = True  # do not drop the start sentence symbol\n",
    "        sent = [w for j, w in enumerate(words) if keep[j]]\n",
    "        sent += [vocab.pad] * (len(words)-len(sent))\n",
    "        x_.append(sent)\n",
    "    return torch.LongTensor(x_).t().contiguous().to(x.device)\n",
    "\n",
    "def word_blank(vocab, x, p):     # blank words with probability p\n",
    "    blank = (torch.rand(x.size(), device=x.device) < p) & \\\n",
    "        (x != vocab.go) & (x != vocab.pad)\n",
    "    x_ = x.clone()\n",
    "    x_[blank] = vocab.blank\n",
    "    return x_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dimensional-sculpture",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = line.split()\n",
    "sen_len = len(word_list)\n",
    "keep = np.random.rand(len(word_list)) > drop_prob\n",
    "sent = [w for j, w in enumerate(word_list) if keep[j]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fourth-separate",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'builtin_function_or_method' object has no attribute 'shuffle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-89e7251eea67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrandom\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mli\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mli\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'builtin_function_or_method' object has no attribute 'shuffle'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "li=list(range(10))\n",
    "random.shuffle(li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "congressional-wilderness",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "a=[1,2,3,4,5,6,7,8,9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "orange-driver",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.8.1'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dutch-humor",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'train.tgt'\n",
    "f_valid_en = open('./e2e/e2e-dataset/'+file)\n",
    "lines_valid = f_valid_en.readlines()\n",
    "f_valid_en.close()\n",
    "import numpy as np\n",
    "import os \n",
    "\n",
    "path = \"e2e_blank2\"\n",
    "drop_prob = 0.2\n",
    "folder = os.path.exists(path)\n",
    "if not folder:                   #判断是否存在文件夹如果不存在则创建为文件夹\n",
    "    os.makedirs(path)            #makedirs 创建文件时如果路径不存在会创建这个路径\n",
    "with open('./'+path+'/'+file, 'a') as the_file:\n",
    "    drop_lines = []\n",
    "    sum_len = 0\n",
    "    for line in lines_valid:\n",
    "        word_list = line.split()\n",
    "        sen_len = len(word_list)\n",
    "        keep = np.random.rand(len(word_list)) > drop_prob\n",
    "#         sent = [w for j, w in enumerate(word_list) if keep[j]]\n",
    "        sent = []\n",
    "        for j,w in enumerate(word_list):\n",
    "            if keep[j]:\n",
    "                sent.append(w)\n",
    "            else:\n",
    "                sent.append('<blank>')\n",
    "#         sent = [word_list[j] for j,k in enumerate(keep) if k else '<blank>']\n",
    "        the_file.write(' '.join(sent)+'\\n')\n",
    "        sum_len+= len(word_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "owned-bulgarian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Aromi', 'is', 'an', 'English', 'restaurant', 'in', 'the', 'city', 'centre.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "specified-finish",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Aromi', 'is', 'an', 'English', 'restaurant', 'in', 'the', 'city', 'centre.']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "fifth-chinese",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "logits = torch.log_softmax(torch.randn(1, 5),dim=1)*5\n",
    "# Sample soft categorical using reparametrization trick:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "considerable-fault",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-12.6465,  -2.1335, -12.8090,  -9.1892, -17.3195]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "nasty-explanation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.gumbel_softmax(logits, tau=1.4, hard=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rural-romantic",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "introductory-coordinate",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
